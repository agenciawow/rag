# buscador_otimizado.py

import os
import re
import base64
import logging
from typing import List, Tuple, Optional
from dotenv import load_dotenv

import voyageai
from openai import OpenAI
from PIL import Image
from astrapy import DataAPIClient

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Configura√ß√µes Otimizadas ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
LLM_MODEL = "gpt-4.1"              # GPT-4.1 (mais inteligente)
MAX_INITIAL_FETCH = 8               # Fase 1: Busca ampla (era 5)
MAX_FINAL_SELECTION = 2             # Fase 2: Sele√ß√£o final
MAX_TOKENS_RERANK = 512            
MAX_TOKENS_ANSWER = 2048           
COLLECTION_NAME = "pdf_documents"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Classe Otimizada ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class OptimizedMultimodalRagSearcher:
    def __init__(self) -> None:
        """Inicializa com GPT-4.1 e l√≥gica de duas fases."""
        load_dotenv()

        required = [
            "VOYAGE_API_KEY", "OPENAI_API_KEY",
            "ASTRA_DB_API_ENDPOINT", "ASTRA_DB_APPLICATION_TOKEN"
        ]
        for k in required:
            if not os.getenv(k):
                raise ValueError(f"Chave {k} n√£o encontrada em .env")

        voyageai.api_key = os.environ["VOYAGE_API_KEY"]
        self.voyage_client = voyageai.Client()
        self.openai_client = OpenAI()

        try:
            logger.info("Conectando ao Astra DB‚Ä¶")
            client = DataAPIClient()
            database = client.get_database(
                os.environ["ASTRA_DB_API_ENDPOINT"], 
                token=os.environ["ASTRA_DB_APPLICATION_TOKEN"]
            )
            self.collection = database.get_collection(COLLECTION_NAME)
            
            try:
                list(self.collection.find({}, limit=1))
                logger.info("‚úÖ Conectado ao Astra DB - Collection '%s' acess√≠vel", COLLECTION_NAME)
            except Exception:
                logger.error("‚ùå Collection '%s' n√£o encontrada ou inacess√≠vel", COLLECTION_NAME)
                raise
                
        except Exception as e:
            logger.error("Falha ao conectar Astra DB: %s", e)
            raise

        logger.info("üöÄ Sistema RAG Otimizado pronto com GPT-4.1!")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Embedding da consulta ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def get_query_embedding(self, query: str) -> List[float]:
        """Gera embedding de texto para a consulta."""
        try:
            res = self.voyage_client.multimodal_embed(
                inputs=[[query]],
                model="voyage-multimodal-3",
                input_type="query"
            )
            return res.embeddings[0]
        except Exception as e:
            logger.error("Erro embedding consulta: %s", e)
            raise

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Utilidades de imagem ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    @staticmethod
    def encode_image_to_base64(image_path: str) -> Optional[str]:
        """Converte imagem local em base64."""
        try:
            if not image_path or not os.path.exists(image_path):
                logger.warning("Imagem n√£o encontrada: %s", image_path)
                return None
            with open(image_path, "rb") as f:
                return base64.b64encode(f.read()).decode("utf-8")
        except Exception as e:
            logger.error("Erro codificando %s: %s", image_path, e)
            return None

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FASE 1: Busca Ampla ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def phase1_broad_search(self, query_embedding: List[float]) -> List[dict]:
        """
        FASE 1: Busca ampla - "Cast a Wide Net"
        Busca mais candidatos para n√£o perder documentos relevantes.
        """
        try:
            logger.info("üåê FASE 1: Busca ampla (top-%d candidatos)...", MAX_INITIAL_FETCH)
            
            cursor = self.collection.find(
                {},
                sort={"$vector": query_embedding},
                limit=MAX_INITIAL_FETCH,  # Busca mais candidatos
                include_similarity=True,
                projection={
                    "file_path": True,
                    "page_num": True,
                    "doc_source": True,
                    "markdown_text": True,
                    "_id": True
                }
            )
            
            candidates = []
            for doc in cursor:
                candidates.append({
                    "file_path": doc.get("file_path"),
                    "page_num": doc.get("page_num"),
                    "doc_source": doc.get("doc_source"),
                    "markdown_text": doc.get("markdown_text", ""),
                    "similarity_score": doc.get("$similarity", 0.0),
                })
            
            logger.info("üìä Fase 1 retornou %d candidatos", len(candidates))
            
            # Log dos scores para an√°lise
            for i, c in enumerate(candidates):
                logger.info("   Candidato %d: P√°gina %d, Score=%.4f", 
                           i+1, c['page_num'], c['similarity_score'])
            
            return candidates
        except Exception as e:
            logger.error("Erro na Fase 1: %s", e)
            return []

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FASE 2: Sele√ß√£o Precisa com GPT-4.1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def phase2_precise_reranking(self, query: str, candidates: List[dict]) -> Tuple[List[dict], str]:
        """
        FASE 2: Sele√ß√£o precisa - "Be Selective"
        Usa GPT-4.1 para escolher os melhores candidatos da Fase 1.
        """
        if not candidates:
            return [], "Nenhum candidato da Fase 1."

        if len(candidates) == 1:
            c = candidates[0]
            doc_name = os.path.basename(c["file_path"]).replace(".png", "")
            return [c], f"√önico candidato {doc_name}, p.{c['page_num']}."

        try:
            logger.info("üéØ FASE 2: Sele√ß√£o precisa com GPT-4.1 (%d ‚Üí %d)...", 
                       len(candidates), MAX_FINAL_SELECTION)
            
            pages_info = ", ".join(
                f"{os.path.basename(c['file_path']).replace('.png','')} (p.{c['page_num']}, score={c['similarity_score']:.3f})"
                for c in candidates
            )
            
            # Prompt otimizado para GPT-4.1
            prompt_head = (
                f"Voc√™ √© um assistente especialista usando GPT-4.1 para sele√ß√£o precisa de documentos.\n\n"
                f"PERGUNTA: '{query}'\n\n"
                f"CANDIDATOS da Fase 1 ({len(candidates)} p√°ginas): {pages_info}\n\n"
                f"TAREFA:\n"
                f"Analise cada p√°gina e selecione APENAS as mais relevantes que contenham informa√ß√£o espec√≠fica para responder √† pergunta.\n"
                f"- Priorize p√°ginas com informa√ß√£o direta e factual\n"
                f"- M√°ximo {MAX_FINAL_SELECTION} p√°ginas\n"
                f"- Se uma p√°gina j√° responde completamente, n√£o selecione outras\n\n"
                f"FORMATO DE RESPOSTA:\n"
                f"P√°ginas_Selecionadas: [n¬∫] ou [n¬∫1, n¬∫2]\n"
                f"Justificativa: Explique brevemente por que essas p√°ginas s√£o as melhores\n"
                f"Confidence: Alta/M√©dia/Baixa"
            )
            
            content = [{"type": "text", "text": prompt_head}]

            # Adicionar contexto visual de cada candidato
            for cand in candidates:
                b64 = self.encode_image_to_base64(cand["file_path"])
                if not b64:
                    continue
                    
                preview = cand["markdown_text"][:400]  # Mais contexto
                text_block = (
                    f"\n=== P√ÅGINA {cand['page_num']} ===\n"
                    f"Documento: {os.path.basename(cand['file_path']).replace('.png','').upper()}\n"
                    f"Similarity Score: {cand['similarity_score']:.4f}\n"
                    f"Conte√∫do: {preview}{'‚Ä¶' if len(cand['markdown_text'])>400 else ''}\n"
                )
                content.append({"type": "text", "text": text_block})
                content.append({"type": "image_url",
                                "image_url": {"url": f"data:image/png;base64,{b64}"}})

            # Usar GPT-4.1 para sele√ß√£o precisa
            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,  # GPT-4.1
                messages=[{"role": "user", "content": content}],
                max_tokens=MAX_TOKENS_RERANK,
                temperature=0.0  # Determin√≠stico para consist√™ncia
            )
            
            result = response.choices[0].message.content or ""
            logger.info("üìù Resposta GPT-4.1 Fase 2: %s", result)

            # Parser da resposta
            selected_nums: List[int] = []
            justification = "Justificativa ausente."
            confidence = "M√©dia"
            
            for line in result.splitlines():
                line = line.strip()
                if line.lower().startswith("p√°ginas_selecionadas"):
                    selected_nums = [int(n) for n in re.findall(r"\d+", line)]
                elif line.startswith("Justificativa:"):
                    justification = line.replace("Justificativa:", "").strip()
                elif line.startswith("Confidence:"):
                    confidence = line.replace("Confidence:", "").strip()

            # Mapear n√∫meros selecionados para candidatos
            chosen = [c for c in candidates if c["page_num"] in selected_nums]
            
            if chosen:
                logger.info("‚úÖ Fase 2 selecionou %d p√°ginas: %s (Confidence: %s)", 
                           len(chosen), [c['page_num'] for c in chosen], confidence)
                return chosen, f"{justification} (Confidence: {confidence})"
            else:
                logger.warning("‚ö†Ô∏è GPT-4.1 n√£o selecionou p√°ginas v√°lidas; usando a mais similar.")
                return [candidates[0]], "Fallback: usando candidato com maior similarity."

        except Exception as e:
            logger.error("Erro na Fase 2: %s", e)
            return [candidates[0]], "Fallback: erro na sele√ß√£o com GPT-4.1."

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Verifica√ß√£o de Relev√¢ncia ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def verify_relevance(self, query: str, selected: List[dict]) -> bool:
        """Verifica se a resposta est√° de fato no contexto selecionado."""
        if not selected:
            return False

        try:
            logger.info("üîç Verifica√ß√£o de relev√¢ncia...")
            context_text = "\n\n".join(
                f"=== P√ÅGINA {c['page_num']} ===\n{c['markdown_text']}"
                for c in selected
            )

            prompt = (
                f"Analise o conte√∫do para responder: \"{query}\"\n\n"
                f"Conte√∫do selecionado:\n---\n{context_text}\n---\n\n"
                f"O conte√∫do cont√©m informa√ß√£o factual e espec√≠fica para responder √† pergunta? "
                f"Considere apenas respostas diretas e expl√≠citas.\n"
                f"Responda apenas: 'Sim' ou 'N√£o'"
            )

            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,  # GPT-4.1 para verifica√ß√£o tamb√©m
                messages=[{"role": "user", "content": prompt}],
                max_tokens=5,
                temperature=0.0
            )
            
            verification_result = response.choices[0].message.content or ""
            logger.info("üìã Verifica√ß√£o de relev√¢ncia: '%s'", verification_result)
            
            return "sim" in verification_result.lower()

        except Exception as e:
            logger.error("Erro na verifica√ß√£o de relev√¢ncia: %s", e)
            return True  # Fallback conservador

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Gera√ß√£o da resposta final com GPT-4.1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def generate_final_answer(self, query: str, selected: List[dict]) -> str:
        """Constr√≥i resposta final usando GPT-4.1."""
        try:
            logger.info("üìù Gerando resposta final com GPT-4.1...")
            
            no_md = "- N√ÉO use formata√ß√£o Markdown como **, _, #. Escreva texto corrido."
            
            if len(selected) == 1:
                c = selected[0]
                doc = os.path.basename(c["file_path"]).split("_page_")[0]
                prompt = (
                    f"Voc√™ √© um assistente especializado usando GPT-4.1 para an√°lise de documentos acad√™micos.\n\n"
                    f"PERGUNTA: {query}\n\n"
                    f"DOCUMENTO: Use APENAS a p√°gina {c['page_num']} do documento '{doc}' abaixo.\n\n"
                    f"CONTE√öDO DA P√ÅGINA:\n{c['markdown_text']}\n\n"
                    f"INSTRU√á√ïES:\n"
                    f"- Responda com base exclusivamente no conte√∫do fornecido\n"
                    f"- Se a resposta estiver presente, explique de forma clara e completa\n"
                    f"- Se n√£o estiver, informe que a informa√ß√£o espec√≠fica n√£o est√° dispon√≠vel\n"
                    f"- Mencione que a resposta vem do documento '{doc}', p√°gina {c['page_num']}\n"
                    f"- Use linguagem clara e precisa\n"
                    f"{no_md}"
                )
                content = [{"type": "text", "text": prompt}]
                
                b64 = self.encode_image_to_base64(c["file_path"])
                if b64:
                    content.append({"type": "image_url",
                                    "image_url": {"url": f"data:image/png;base64,{b64}", "detail": "high"}})

            else:
                # M√∫ltiplas p√°ginas
                pages_str = " e ".join(
                    f"{os.path.basename(c['file_path']).split('_page_')[0]} p.{c['page_num']}"
                    for c in selected
                )
                combined_text = "\n\n".join(
                    f"=== P√ÅGINA {c['page_num']} ===\n{c['markdown_text']}"
                    for c in selected
                )
                prompt = (
                    f"Voc√™ √© um assistente especializado usando GPT-4.1.\n\n"
                    f"PERGUNTA: {query}\n\n"
                    f"DOCUMENTOS: Use APENAS as p√°ginas {pages_str} abaixo.\n\n"
                    f"CONTE√öDO COMBINADO:\n{combined_text}\n\n"
                    f"INSTRU√á√ïES:\n"
                    f"- Integre informa√ß√µes de todas as p√°ginas relevantes\n"
                    f"- Seja claro sobre qual p√°gina cont√©m cada informa√ß√£o\n"
                    f"- Se alguma informa√ß√£o estiver ausente, mencione explicitamente\n"
                    f"- Cite as p√°ginas utilizadas na resposta\n"
                    f"{no_md}"
                )
                content = [{"type": "text", "text": prompt}]
                
                for c in selected:
                    b64 = self.encode_image_to_base64(c["file_path"])
                    if b64:
                        content.append({"type": "text", "text": f"\n--- IMAGEM P√ÅGINA {c['page_num']} ---"})
                        content.append({"type": "image_url",
                                        "image_url": {"url": f"data:image/png;base64,{b64}", "detail": "high"}})

            # GPT-4.1 para resposta final
            response = self.openai_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": content}],
                max_tokens=MAX_TOKENS_ANSWER,
                temperature=0.1
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error("Erro gerando resposta final: %s", e, exc_info=True)
            return f"Erro ao gerar resposta com GPT-4.1: {e}"

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Pipeline Completo de Duas Fases ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def search_and_answer(self, query: str) -> dict:
        """Pipeline otimizado com duas fases: Busca Ampla ‚Üí Sele√ß√£o Precisa."""
        logger.info("üöÄ Iniciando pipeline otimizado de duas fases...")
        logger.info("üìù Consulta: '%s'", query)
        
        try:
            embedding = self.get_query_embedding(query)
        except Exception as e:
            return {"error": f"Embedding falhou: {e}"}

        # FASE 1: Busca ampla
        candidates = self.phase1_broad_search(embedding)
        if not candidates:
            return {"error": "Fase 1: Nenhuma p√°gina relevante encontrada."}

        # FASE 2: Sele√ß√£o precisa com GPT-4.1
        selected, justification = self.phase2_precise_reranking(query, candidates)
        if not selected:
            return {"error": "Fase 2: Falha na sele√ß√£o precisa."}

        # Verifica√ß√£o de relev√¢ncia
        if not self.verify_relevance(query, selected):
            logger.warning(
                "‚ùå Verifica√ß√£o de relev√¢ncia indicou que a resposta n√£o est√° no contexto selecionado. "
                "Interrompendo para evitar resposta incorreta."
            )
            return {
                "error": "A informa√ß√£o solicitada n√£o foi encontrada de forma expl√≠cita no documento."
            }

        # Gera√ß√£o da resposta final
        logger.info("üìù Gerando resposta final...")
        answer = self.generate_final_answer(query, selected)

        # Preparar detalhes da resposta
        sel_details = [
            {
                "document": os.path.basename(c["file_path"]).split("_page_")[0],
                "page_number": c["page_num"],
                "similarity_score": c["similarity_score"],
            }
            for c in selected
        ]
        
        all_details = [
            {
                "document": os.path.basename(c["file_path"]).split("_page_")[0],
                "page_number": c["page_num"],
                "similarity_score": c["similarity_score"],
            }
            for c in candidates
        ]
        
        sel_str = " + ".join(
            f"{p['document']} (p.{p['page_number']})" for p in sel_details
        )

        return {
            "query": query,
            "selected_pages": sel_str,
            "selected_pages_details": sel_details,
            "selected_pages_count": len(selected),
            "justification": justification,
            "answer": answer,
            "total_candidates": len(candidates),
            "all_candidates": all_details,
            "pipeline_info": {
                "phase1_candidates": len(candidates),
                "phase2_selected": len(selected),
                "model_used": LLM_MODEL,
                "optimization": "Two-Phase Retrieval"
            }
        }

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Interface CLI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def main() -> None:
    try:
        searcher = OptimizedMultimodalRagSearcher()
        print("üöÄ RAG OTIMIZADO - Two-Phase Retrieval com GPT-4.1 üöÄ")
        print("=" * 70)
        print("üìä Configura√ß√£o: Fase 1 (top-8) ‚Üí Fase 2 (top-2) ‚Üí GPT-4.1")
        print("=" * 70)

        while True:
            user_q = input("üí¨ Sua pergunta: ").strip()
            if user_q.lower() in {"sair", "exit", "quit"}:
                print("üëã At√© logo!")
                break
            if not user_q:
                continue

            print("\n" + "‚îÄ" * 70 + "\nüîç Processando com pipeline de duas fases...")
            result = searcher.search_and_answer(user_q)

            if "error" in result:
                print("‚ùå", result["error"])
                continue

            print(f"\nüìÑ P√°ginas selecionadas: {result['selected_pages']}")
            print(f"ü§ñ Justificativa: {result['justification']}")
            
            pipeline_info = result.get('pipeline_info', {})
            print(f"‚öôÔ∏è Pipeline: {pipeline_info.get('phase1_candidates', 0)} ‚Üí {pipeline_info.get('phase2_selected', 0)} p√°ginas")
            
            print("\nüìù RESPOSTA:\n" + "‚ïê" * 70)
            print(result["answer"])
            print("‚ïê" * 70 + "\n")

    except KeyboardInterrupt:
        print("\nüëã At√© logo!")
    except Exception as e:
        logger.critical("Erro fatal: %s", e, exc_info=True)
        print("‚ùå Erro fatal:", e)

__all__ = ['OptimizedMultimodalRagSearcher']

if __name__ == "__main__":
    main()